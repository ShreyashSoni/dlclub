{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data.csv')\n",
    "classes = dataframe['class']\n",
    "x_values = dataframe['x']\n",
    "y_values = dataframe['y']\n",
    "\n",
    "num_arr = dataframe.values\n",
    "x_v = num_arr[:, 0:2]\n",
    "y_v = num_arr[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function fot the sigmoid activation function.\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Function for the derivative of sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Function to make predictions.\n",
    "def prediction(X, W, b):\n",
    "    return sigmoid(np.matmul(X,W)+b)\n",
    "\n",
    "# Function to calculate the cross entropy\n",
    "def error_vector(y, y_hat):\n",
    "    return [-y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i]) for i in range(len(y))]\n",
    "\n",
    "# Function to calculate the total error.\n",
    "def error(y, y_hat):\n",
    "    ev = error_vector(y, y_hat)\n",
    "    return sum(ev)/len(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the gradient of the error function.\n",
    "# The result is a list of three lists:\n",
    "# The first list contains the gradient with respect to w1.\n",
    "# The second list contains the gradient with respect to w2.\n",
    "# The third list contains the gradient with respect to b.\n",
    "def dErrors(X, y, y_hat):\n",
    "    deDx1 = [X[i][0]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    deDx2 = [X[i][1]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    deDb = [y[i]-y_hat[i] for i in range(len(y))]\n",
    "    return deDx1, deDx2, deDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to implement the gradient descent step.\n",
    "# Input params: data X, the labels y, the weights W (as an array) and the bias b.\n",
    "# It calculates the prediction, the gradients and use them to update the weights and bias W, b.\n",
    "# It returns W and b and error.\n",
    "def gradientDescentStep(X, y, W, b, learn_rate = 0.01):\n",
    "    y_hat = prediction(X, W, b)\n",
    "    errors = error_vector(y, y_hat)\n",
    "    derivErrors = dErrors(X, y, y_hat)\n",
    "    W[0] += sum(derivErrors[0]) * learn_rate\n",
    "    W[1] += sum(derivErrors[1]) * learn_rate\n",
    "    b += sum(derivErrors[2]) * learn_rate\n",
    "    return W, b, sum(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function runs thr perceptron algorithm repeatedly on the dataset,\n",
    "# and returns the boundary lines obtained in the iterations for plotting.\n",
    "# Feel free to play with the learning rate and num_epochs,\n",
    "# See your results plotted below.\n",
    "def trainLR(X, y, learn_rate = 0.01, num_epochs = 100):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    # Initialize the weights randomly\n",
    "    W = np.array(np.random.rand(2,1))*2 -1\n",
    "    b = np.random.rand(1)[0]*2 -1\n",
    "    # These are solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    errors = []\n",
    "    for i in range(num_epochs):\n",
    "        # in each epoch, we apply the gradient descent step.\n",
    "        W, b, error = gradientDescentStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "        errors.append(error)\n",
    "    return boundary_lines, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_l, e_l = trainLR(x_v, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xi = np.arange(-0.5, 1.5, 0.25)\n",
    "for i in range (0, len(b_l)-1):\n",
    "    line = xi*b_l[i][0] + b_l[i][1]\n",
    "    plt.plot(xi, line, linestyle='--', color='green')\n",
    "    \n",
    "line = xi*b_l[len(b_l)-1][0] + b_l[len(b_l)-1][1]\n",
    "plt.plot(xi, line, color='black')\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.scatter(x_values[0:50], y_values[0:50], color='blue')\n",
    "plt.scatter(x_values[50:100], y_values[50:100], color='red')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(e_l)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
